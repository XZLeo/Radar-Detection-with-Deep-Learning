{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how many empty scenes?\n",
    "# how many objects with only one or two points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from typing import Sequence\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np    \n",
    "from radar_scenes.coordinate_transformation import *\n",
    "from radar_scenes.sequence import Sequence\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestamps(sequence: Sequence):\n",
    "    '''\n",
    "    Create the list of all timesteps\n",
    "    '''\n",
    "    timestamps = []\n",
    "    for idx, scene in enumerate(sequence.scenes()):\n",
    "        radar_data = scene.radar_data\n",
    "        timestamps.append(radar_data[0][0]) \n",
    "    return timestamps\n",
    "        \n",
    "\n",
    "def get_current_scenes(sequence: Sequence, cur_idx, timestamps, n_prev_frames:int, n_next_frames: int): # what is timestamps?? \n",
    "    \"\"\"\n",
    "    Retrieves the scenes which should be displayed according to the current values of the time slider and the\n",
    "    spinboxes for the past and future frames.\n",
    "    Values of the spinboxes are retrieved and from the list of timestamps, the corresponding times are obtained.\n",
    "    :return: The current frame (type Scene) and a list of other frames (type Scene) which should be displayed.\n",
    "    \"\"\"\n",
    "    # cur_idx = timeline_slider.value()\n",
    "    cur_timestamp = timestamps[cur_idx]\n",
    "    current_scene = sequence.get_scene(cur_timestamp)\n",
    "    other_scenes = []\n",
    "    for i in range(1, n_prev_frames + 1):  # in the GUI, show previous frames/ show future frames!\n",
    "        if cur_idx - i < 0:\n",
    "            break\n",
    "        t = timestamps[cur_idx - i]\n",
    "        other_scenes.append(sequence.get_scene(t))\n",
    "    for i in range(1, n_next_frames + 1):\n",
    "        if cur_idx + i >= len(timestamps):\n",
    "            break\n",
    "        t = timestamps[cur_idx + i]\n",
    "        other_scenes.append(sequence.get_scene(t))\n",
    "    return current_scene, other_scenes\n",
    "\n",
    "\n",
    "def trafo_radar_data_world_to_car(scene, other_scenes) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Transforms the radar data listed in other_scenes into the same car coordinate system that is used in 'scene'.\n",
    "    :param scene: Scene. Containing radar data and odometry information of one scene. The odometry information from\n",
    "    this scene is used to transform the detections from the other timestamps into this scene.\n",
    "    :param other_scenes: List of Scene items. All detections in these other scenes are transformed\n",
    "    :return: A numpy array with all radar data from all scenes. The fields \"x_cc\" and \"y_cc\" are now relative to the\n",
    "    current scene.\n",
    "    \"\"\"\n",
    "    if len(other_scenes) == 0:\n",
    "        return scene.radar_data\n",
    "    other_radar_data = np.hstack([x.radar_data for x in other_scenes])\n",
    "    x_cc, y_cc = transform_detections_sequence_to_car(other_radar_data[\"x_seq\"], other_radar_data[\"y_seq\"],\n",
    "                                                        scene.odometry_data)\n",
    "    other_radar_data[\"x_cc\"] = x_cc\n",
    "    other_radar_data[\"y_cc\"] = y_cc\n",
    "    return np.hstack([scene.radar_data, other_radar_data])\n",
    "\n",
    "\n",
    "def get_frames(sequence: Sequence, cur_idx, timestamps, n_prev_frames=0 , n_next_frames=0):\n",
    "    \"\"\"\n",
    "    Plot the current frames.\n",
    "    :param: cur_idx: the frame number to be ploted\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if len(timestamps) == 0 or cur_idx >= len(timestamps):\n",
    "        return\n",
    "    cur_timestamp = timestamps[cur_idx]\n",
    "    current_scene, other_scenes = get_current_scenes(sequence, cur_idx, timestamps, n_prev_frames, n_next_frames)   # 4 sensors together\n",
    "    radar_data = trafo_radar_data_world_to_car(current_scene, other_scenes) \n",
    "    return radar_data\n",
    "\n",
    "def plot_frames(radar_data: list):\n",
    "    # extract x, y from the list\n",
    "    x = np.zeros(len(radar_data))\n",
    "    y = np.zeros(len(radar_data))\n",
    "    for idx, point in enumerate(radar_data): # radar_data is ndarray already, can be simplified here !\n",
    "        x[idx] = point[7]\n",
    "        y[idx] = point[8]\n",
    "    col = [0, 0, 0, 1]\n",
    "    plt.plot(\n",
    "        y,\n",
    "        x,\n",
    "        \"o\",\n",
    "        markerfacecolor=tuple(col),\n",
    "        markeredgecolor=\"k\",\n",
    "        markersize=3\n",
    "    )\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.gca().invert_xaxis()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# statistic for the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_to_dataset = \"/home/s0001516/thesis/dataset/data\"\n",
    "# list all the folders\n",
    "list_sequences = os.listdir(path_to_dataset)\n",
    "nm_labeled_points = []\n",
    "nm_instances = []\n",
    "# iterate over all sequences\n",
    "nm = 0\n",
    "for nm_sequence in list_sequences:\n",
    "    path_sequence = os.path.join(path_to_dataset, nm_sequence)\n",
    "    if os.path.isdir(path_sequence):\n",
    "        # Define the *.json file from which data should be loaded\n",
    "        filename = os.path.join(path_sequence, \"scenes.json\")\n",
    "        sequence = Sequence.from_json(filename)\n",
    "        timestamps = get_timestamps(sequence)\n",
    "        # iterate over all sensor scans\n",
    "        for cur_idx in range(len(timestamps)):\n",
    "            #m number of labeled points (not background)\n",
    "            radar_data = get_frames(sequence, cur_idx, timestamps)\n",
    "            labeled_idx = np.where(radar_data[\"label_id\"] != 11)[0] # not static # 11 means static points, background\n",
    "            idx = radar_data.shape[0]\n",
    "            nm += idx\n",
    "            nm_labeled_points.append(len(labeled_idx))\n",
    "            # number of instances\n",
    "            track_ids = set(radar_data[\"track_id\"])\n",
    "            nm_instances.append(len(track_ids)-1)\n",
    "\n",
    "print('Number of total sensor scans is {}'.format(len(nm_labeled_points)))\n",
    "print('Number of total points is {}'.format(nm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of total points is {}'.format(len(nm_labeled_points)))\n",
    "#print(nm_labeled_points)\n",
    "print(len(track_ids))  # total number of unique objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: replot the histogram as Fig 4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "plt.hist(np.array(nm_labeled_points), bins=np.arange(1, 50, 5), facecolor=\"blue\", edgecolor=\"black\", alpha=0.7)\n",
    "# 显示横轴标签\n",
    "plt.xlabel(\"number of labeled points\", fontsize=15)\n",
    "# 显示纵轴标签\n",
    "plt.ylabel(\"frequency\", fontsize=15)\n",
    "# 显示图标题\n",
    "plt.title(\"labled points per scan\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how many sensor scan that has no labeled points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sum(np.array(nm_labeled_points)==0)\n",
    "b = sum(np.array(nm_labeled_points)==1)\n",
    "print('There are {} / {} scan with no labeled points \\n {}'.format(a, len(nm_labeled_points), a/len(nm_labeled_points)))\n",
    "print('There are {} / {} scan with one labeled points \\n {}'.format(b, len(nm_labeled_points), b/len(nm_labeled_points)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} / {} scan with no labeled points or with only one labelled points \\n {}'.format(a+b, len(nm_labeled_points), (a+b)/len(nm_labeled_points)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks quite close to 4 b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "y1 = sum(np.array(nm_instances) == 1)\n",
    "y2 = sum(np.array(nm_instances) == 2)\n",
    "y3 = sum(np.array(nm_instances) == 3)\n",
    "y4 = sum(np.array(nm_instances) == 4)\n",
    "print(y1)\n",
    "print(y2)\n",
    "print(y3)\n",
    "print(y4)\n",
    "x = range(1, np.max(nm_instances))\n",
    "y = []\n",
    "for i in x:\n",
    "    y.append(sum(np.array(nm_instances) == i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.bar(x, y)\n",
    "# 显示横轴标签\n",
    "plt.xlabel(\"number of instances\", fontsize=15)\n",
    "# 显示纵轴标签\n",
    "plt.ylabel(\"frequency\", fontsize=15)\n",
    "# 显示图标题\n",
    "plt.title(\"instances per scan\", fontsize=15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(y)) # totoal number of instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array([radar_data['x_cc'], radar_data['y_cc']]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many frames does a 500ms-snippet contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time interval distribution\n",
    "\n",
    "path_to_dataset = \"/home/s0001516/thesis/dataset/data\"\n",
    "# list all the folders\n",
    "list_sequences = os.listdir(path_to_dataset)\n",
    "intervals = []\n",
    "# iterate over all sequences\n",
    "for nm_sequence in list_sequences:\n",
    "    path_sequence = os.path.join(path_to_dataset, nm_sequence)\n",
    "    if os.path.isdir(path_sequence):\n",
    "        # Define the *.json file from which data should be loaded\n",
    "        filename = os.path.join(path_sequence, \"scenes.json\")\n",
    "        sequence = Sequence.from_json(filename)\n",
    "        timestamps = get_timestamps(sequence)\n",
    "        intervals += list(np.array(timestamps[1:-1])-np.array(timestamps[0:-2])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.array(intervals), bins=np.arange(1, 100000, 1000), facecolor=\"blue\", edgecolor=\"black\", alpha=0.7)\n",
    "# 显示横轴标签\n",
    "plt.xlabel(\"time interval/us\", fontsize=15)\n",
    "# 显示纵轴标签\n",
    "plt.ylabel(\"frequency\", fontsize=15)\n",
    "# 显示图标题\n",
    "plt.title(\"frequency of time interval\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"maximum time interval {}\".format(max(intervals)))\n",
    "print(\"minimum time interval {}\".format(min(intervals)))\n",
    "print(\"median time interval {}\".format(np.median(intervals)))\n",
    "counts = np.bincount(intervals)\n",
    "print(\"most frequent time interval {}\".format(np.argmax(counts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non overlapping\n",
    "\n",
    "path_to_dataset = \"/home/s0001516/thesis/dataset/data\"\n",
    "# list all the folders\n",
    "list_sequences = os.listdir(path_to_dataset)\n",
    "counts = []\n",
    "# iterate over all sequences\n",
    "for nm_sequence in list_sequences:\n",
    "    path_sequence = os.path.join(path_to_dataset, nm_sequence)\n",
    "    if os.path.isdir(path_sequence):\n",
    "        # Define the *.json file from which data should be loaded\n",
    "        filename = os.path.join(path_sequence, \"scenes.json\")\n",
    "        sequence = Sequence.from_json(filename)\n",
    "        timestamps = get_timestamps(sequence)\n",
    "        intervals = list(np.array(timestamps[1:-1])-np.array(timestamps[0:-2])) \n",
    "        sum = 0\n",
    "        count = 0\n",
    "        for interval in intervals:\n",
    "            if sum < 500000:\n",
    "                sum += interval\n",
    "                count += 1\n",
    "            else:\n",
    "                counts.append(count)\n",
    "                sum = 0\n",
    "                count = 0\n",
    "\n",
    "\n",
    "print(\"maximum number of scans {}\".format(max(counts)))\n",
    "print(\"minimum number of scans {}\".format(min(counts)))\n",
    "print(\"median number of scans {}\".format(np.median(counts)))\n",
    "x = np.bincount(counts)\n",
    "print(\"most frequent number of scans {}\".format(np.argmax(x)))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# what is the distribution of track-id over time looks like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dataset = \"/home/s0001516/thesis/dataset/RadarScenes/data\"\n",
    "# list all the folders\n",
    "list_sequences = os.listdir(path_to_dataset)\n",
    "nm_labeled_points = []\n",
    "nm_instances = []\n",
    "# iterate over all sequences\n",
    "for nm_sequence in list_sequences:\n",
    "    path_sequence = os.path.join(path_to_dataset, nm_sequence)\n",
    "    if os.path.isdir(path_sequence):\n",
    "        # Define the *.json file from which data should be loaded\n",
    "        filename = os.path.join(path_sequence, \"scenes.json\")\n",
    "        sequence = Sequence.from_json(filename)\n",
    "        timestamps = get_timestamps(sequence)\n",
    "        # iterate over all sensor scans\n",
    "        for cur_idx in range(len(timestamps)):\n",
    "            # number of instances\n",
    "            track_ids = set(radar_data[\"track_id\"])\n",
    "            for track in track_ids:\n",
    "                if radar_data[\"track_id\"]:############? use panda!  determine track_id's index, use it to sort time stamp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doppler Value Skew and Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dataset = \"/home/s0001516/thesis/dataset/data\"\n",
    "# list all the folders\n",
    "list_sequences = os.listdir(path_to_dataset)\n",
    "vr = np.array([0])\n",
    "# iterate over all sequences\n",
    "for nm_sequence in list_sequences:\n",
    "    path_sequence = os.path.join(path_to_dataset, nm_sequence)\n",
    "    if os.path.isdir(path_sequence):\n",
    "        # Define the *.json file from which data should be loaded\n",
    "        filename = os.path.join(path_sequence, \"scenes.json\")\n",
    "        sequence = Sequence.from_json(filename)\n",
    "        timestamps = get_timestamps(sequence)\n",
    "        # iterate over all sensor scans\n",
    "        sampled_idxes = np.random.randint(0, len(timestamps), size=200) #100/7291\n",
    "        for cur_idx in sampled_idxes:\n",
    "            # absolute value of radial velocity\n",
    "            radar_data = get_frames(sequence, cur_idx, timestamps)\n",
    "            vr = np.hstack((vr, np.abs(radar_data[\"vr_compensated\"])))\n",
    "\n",
    "vr = np.delete(vr, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skew_vr(vr):\n",
    "    '''\n",
    "    Using the depicted polynomial scaling function, the distribution is widened \n",
    "    in order to ease the feature extraction process\n",
    "\n",
    "    in fig 7, a fourth order polynomial is used\n",
    "    '''\n",
    "    x = np.arange(0, 3.00, 0.25)\n",
    "    y = np.hstack((np.array([0, 3/4, 15/16, 31/32]), np.linspace(1, 2.75, 8)))\n",
    "    g = interp1d(x, y, kind=\"cubic\")\n",
    "    new_vr = 40 * g(vr/40)\n",
    "    return new_vr     \n",
    "\n",
    "print(max(vr))\n",
    "new_vr = skew_vr(vr)\n",
    "print(max(new_vr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m = plt.hist(new_vr, bins=np.arange(0, 100, 0.5), facecolor=\"red\", edgecolor=\"red\", alpha=0.7)\n",
    "n = plt.hist(vr, bins=np.arange(0, 100, 1), facecolor=\"blue\", edgecolor=\"blue\", alpha=0.7)\n",
    "plt.xlabel(\"radia velocity m/s\")\n",
    "plt.ylabel(\"frequency of radial velocity\")\n",
    "plt.yscale('log')\n",
    "plt.title(\"frequency of radial velocity\")\n",
    "plt.legend(['skewed Vr distribution', 'origin distribution'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skew function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "x = np.arange(0, 3.00, 0.25)\n",
    "y = np.hstack((np.array([0, 3/4, 15/16, 31/32]), np.linspace(1, 2.75, 8)))\n",
    "g = interpolate.interp1d(x, y, kind=\"cubic\")\n",
    "\n",
    "f = lambda x : np.sqrt(x)\n",
    "a = np.arange(0, 2, 0.01)\n",
    "plt.plot(40*a, 40*f(a), '-')\n",
    "plt.plot(40*a, 40*g(a), 'r-')\n",
    "plt.plot(40*a, 40*a, 'g-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nicolas' origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def skew_vr_origin(im):\n",
    "    sx = np.array([0, 10, 20, 27.5, 40])\n",
    "    sy = np.array([0, 0.7, 0.9, 0.95, 1])\n",
    "    poly = np.poly1d(np.polyfit(sx, sy, 4))  # degree of freedom\n",
    "    lin = np.poly1d(np.polyfit(sx[-2:], sy[-2:], 1))\n",
    "\n",
    "    poly_msk = im < 27.5\n",
    "    lin_msk = im >= 27.5\n",
    "    im[poly_msk] = poly(im[poly_msk])\n",
    "    im[lin_msk] = lin(im[lin_msk])\n",
    "\n",
    "    im = np.clip(im, a_min=0.0, a_max=1.0, out=im)\n",
    "    im[np.isclose(im, 0)] = 0\n",
    "    return 40*im\n",
    "\n",
    "def skew_vr_int(im):\n",
    "    sx = np.array([0, 10, 20, 27.5, 40])\n",
    "    sy = np.array([0, 0.7, 0.9, 0.95, 1])\n",
    "    poly = np.poly1d(np.polyfit(sx, sy, 4))\n",
    "    lin = np.poly1d(np.polyfit(sx[-2:], sy[-2:], 1))\n",
    "\n",
    "    if im < 27.5:\n",
    "        im = poly(im)\n",
    "    else:\n",
    "        im = lin(im)\n",
    "\n",
    "    # im = np.clip(im, a_min=0.0, a_max=1.0, out=im)\n",
    "    # im[np.isclose(im, 0)] = 0\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(0, 2, 0.01)\n",
    "b =  (40 * a).tolist()\n",
    "\n",
    "plt.plot(40*a, 40*np.array(list(map(skew_vr_int, b))), 'b-')\n",
    "#plt.plot(40*a, 40*g(a), 'r-')\n",
    "plt.plot(40*a, 40*a, 'g--')\n",
    "plt.xlabel(\"Original velocity m/s\", fontsize=15)\n",
    "plt.ylabel(\"Skewed velocity m/s\", fontsize=15)\n",
    "plt.title(\"Velocity skew function\", fontsize=15)\n",
    "plt.legend(['skew function', '$y=x$'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = plt.hist(vr, bins=np.arange(0, 100, 1), facecolor=\"blue\", edgecolor=\"blue\", alpha=0.7)\n",
    "new_vr = skew_vr_origin(vr)\n",
    "m = plt.hist(new_vr, bins=np.arange(0, 100, 0.5), facecolor=\"red\", edgecolor=\"red\", alpha=0.7)\n",
    "plt.xlabel(\"radia velocity m/s\")\n",
    "plt.ylabel(\"frequency of radial velocity\")\n",
    "plt.yscale('log')\n",
    "plt.title(\"frequency of radial velocity\")\n",
    "plt.legend(['origin distribution', 'skewed Vr distribution'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are all the scenes from a same sensor uniformly-distributed? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_sequence = \"/home/s0001516/thesis/dataset/RadarScenes/train/sequence_17\"\n",
    "intervals = []\n",
    "if os.path.isdir(path_sequence):\n",
    "    # Define the *.json file from which data should be loaded\n",
    "    filename = os.path.join(path_sequence, \"scenes.json\")\n",
    "    sequence = Sequence.from_json(filename)\n",
    "    timestamps = get_timestamps(sequence)\n",
    "    start_timestamp = sequence.first_timestamp\n",
    "    print(start_timestamp)\n",
    "    #print sensor id\n",
    "    timestamp = sequence.next_timestamp_after(start_timestamp)\n",
    "    print('sensor id {}'.format( sequence.get_scene(timestamp).sensor_id))\n",
    "    while timestamp in timestamps:\n",
    "        next_timestamp = sequence.next_timestamp_after(timestamp, same_sensor=True)\n",
    "        if next_timestamp == None:\n",
    "            break\n",
    "        intervals.append(next_timestamp - timestamp)\n",
    "        timestamp = next_timestamp\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m = plt.hist(intervals, bins=np.arange(0, 1e5, 100), facecolor=\"red\", edgecolor=\"red\", alpha=0.7)\n",
    "plt.xlabel(\"time interval for each scan from sensor id 3\")\n",
    "plt.ylabel(\"frequency\")\n",
    "#plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.median( intervals))\n",
    "print(np.mean( intervals))\n",
    "print(np.min(intervals))\n",
    "print(np.max(intervals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a table to store how many objects in each class per sequence\n",
    "# 158 rows, 5 columns\n",
    "# use snippet.py to generate data.txt first\n",
    "from labels import ClassificationLabel\n",
    "from dataset import BasicDataset\n",
    "from frame import get_frames, get_timestamps\n",
    "from numpy import ndarray, zeros, where, savetxt, sum, array\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "class BalanceDataset(BasicDataset):\n",
    "    def __init__(self, dataset_path, snip_path):\n",
    "        super().__init__(dataset_path, snip_path)\n",
    "        self.table = zeros(158,\n",
    "                  dtype=[('CAR', 'int'), ('PEDESTRIAN', 'int'), ('PEDESTRIAN_GROUP', 'int'), \n",
    "                        ('TWO_WHEELER', 'int'), ('LARGE_VEHICLE', 'int')])\n",
    "\n",
    "    def count_class(self):\n",
    "        for line, cur_seq in enumerate(self.list_sequence):\n",
    "            seq_name = cur_seq.sequence_name\n",
    "            seq_idx = int(seq_name.split(\"_\")[1])\n",
    "            timestamps = get_timestamps(cur_seq)\n",
    "            # iterate over snippet\n",
    "            list_start_idx = self.list_start_idx[line]\n",
    "            list_num_frame = self.list_num_future_frames[line]\n",
    "            for i, start in enumerate(list_start_idx):\n",
    "                num_frame = list_num_frame[i]\n",
    "                snip = get_frames(cur_seq, start, \n",
    "                            timestamps, n_next_frames=num_frame)\n",
    "                # iterate over snippets\n",
    "                track_ids = set(snip[\"track_id\"])\n",
    "                for tr_id in track_ids:\n",
    "                    if tr_id != b'':\n",
    "                        idx = where(snip[\"track_id\"] == tr_id)[0]\n",
    "                        num_points = len(idx)\n",
    "                        class_label = snip[idx[0]]['label_id']\n",
    "                        mapped_class_label = ClassificationLabel.label_to_clabel(class_label).value\n",
    "                        if mapped_class_label != 5:\n",
    "                            self.table[line][mapped_class_label] += 1                                           \n",
    "                            \n",
    "    def write_table(self):\n",
    "        #print(self.table)\n",
    "        savetxt('./table.txt', self.table, fmt='%d')\n",
    "\n",
    "    def get_histogram(self):\n",
    "        self.table\n",
    "        hist = []\n",
    "        for name in ['CAR', 'PEDESTRIAN', 'PEDESTRIAN_GROUP', 'TWO_WHEELER', 'LARGE_VEHICLE']:\n",
    "            a = sum(self.table[name])\n",
    "            print(\"{}:{}\".format(name, a))\n",
    "            hist.append(a)\n",
    "        # plot\n",
    "        x = [1,2,3,4,5]\n",
    "        x_label=['CAR', 'PEDESTRIAN', 'PEDESTRIAN\\n_GROUP', 'TWO\\n_WHEELER', 'LARGE\\n_VEHICLE']\n",
    "        plt.xticks(x, x_label)\n",
    "        plt.bar(x, hist)\n",
    "        plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAR:47366\n",
      "PEDESTRIAN:23537\n",
      "PEDESTRIAN_GROUP:29447\n",
      "TWO_WHEELER:6872\n",
      "LARGE_VEHICLE:8753\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "snip_path = 'static/data.txt'\n",
    "dataset_path = \"/home/s0001516/thesis/dataset/data/\"\n",
    "dataset = BalanceDataset(dataset_path, snip_path)\n",
    "dataset.count_class()\n",
    "dataset.get_histogram()\n",
    "dataset.write_table()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snip_path = 'static/test0.txt'\n",
    "dataset_path = \"/home/s0001516/thesis/dataset/RadarScenes_GPU/test\"\n",
    "dataset = BalanceDataset(dataset_path, snip_path)\n",
    "dataset.count_class()\n",
    "dataset.get_histogram()\n",
    "dataset.write_table()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snip_path = 'static/train_full.txt'\n",
    "dataset_path = \"/home/s0001516/thesis/dataset/RadarScenes_GPU/train_full\"\n",
    "dataset = BalanceDataset(dataset_path, snip_path)\n",
    "dataset.count_class()\n",
    "dataset.get_histogram()\n",
    "dataset.write_table()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistic Per Class\n",
    "- what are the mean and variance of point numbers in a cluster of a class?\n",
    "- what are the mean and variance of RCS in a cluster of a class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the mean and std of point numbers with Welford's algorithm\n",
    "https://jonisalonen.com/2013/deriving-welfords-method-for-computing-variance/  \n",
    "Get the mean, standard deviation of the number of the cluster points for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from labels import ClassificationLabel\n",
    "from dataset import BasicDataset\n",
    "from frame import get_frames, get_timestamps\n",
    "from numpy import ndarray, zeros, where, savetxt, sum, array, sqrt\n",
    "#from matplotlib import pyplot as plt\n",
    "from snippet import clip\n",
    "import numba as nb\n",
    "\n",
    "class CountDataset(BasicDataset):\n",
    "    def __init__(self, dataset_path, snip_path):\n",
    "        super().__init__(dataset_path, snip_path)\n",
    "        self.table = zeros(5,  # num, mean, std, max, min\n",
    "                  dtype=[('CAR', 'float'), ('PEDESTRIAN', 'float'), ('PEDESTRIAN_GROUP', 'float'), \n",
    "                        ('TWO_WHEELER', 'float'), ('LARGE_VEHICLE', 'float')])\n",
    "        self.table[4] = (10., 10., 10., 10., 10.)\n",
    "        self.rcs_table = zeros(5,  # num, mean, std, max, min\n",
    "            dtype=[('CAR', 'float'), ('PEDESTRIAN', 'float'), ('PEDESTRIAN_GROUP', 'float'), \n",
    "                ('TWO_WHEELER', 'float'), ('LARGE_VEHICLE', 'float')])\n",
    "        self.rcs_table[4] = (10., 10., 10., 10., 10.)\n",
    " \n",
    "    #@nb.njit\n",
    "    def count_points_class(self):\n",
    "        for line, cur_seq in enumerate(self.list_sequence):\n",
    "            timestamps = get_timestamps(cur_seq)\n",
    "            # iterate over snippet\n",
    "            list_start_idx = self.list_start_idx[line]\n",
    "            list_num_frame = self.list_num_future_frames[line]\n",
    "            for i, start in enumerate(list_start_idx):\n",
    "                num_frame = list_num_frame[i]\n",
    "                snip = get_frames(cur_seq, start, \n",
    "                            timestamps, n_next_frames=num_frame)\n",
    "                snip = clip(snip)\n",
    "                # iterate over clusters\n",
    "                track_ids = set(snip[\"track_id\"])\n",
    "                for tr_id in track_ids:\n",
    "                    if tr_id != b'':\n",
    "                        idx = where(snip[\"track_id\"] == tr_id)[0]\n",
    "                        num_points = len(idx) # number of points\n",
    "                        class_label = snip[idx[0]]['label_id']\n",
    "                        mapped_class_label = ClassificationLabel.label_to_clabel(class_label).value\n",
    "                        if mapped_class_label != 5:\n",
    "                            # get max/min num points\n",
    "                            max_point = self.table[3][mapped_class_label]\n",
    "                            min_point = self.table[4][mapped_class_label]\n",
    "                            if max_point < num_points:\n",
    "                                self.table[3][mapped_class_label] = num_points\n",
    "                            if min_point > num_points:\n",
    "                                self.table[4][mapped_class_label] = num_points\n",
    "                            # get number of instances\n",
    "                            self.table[0][mapped_class_label] += 1\n",
    "                            k = self.table[0][mapped_class_label]\n",
    "                            # get mean and std\n",
    "                            mean = self.table[1][mapped_class_label]\n",
    "                            std = self.table[2][mapped_class_label]\n",
    "                            old_mean = mean\n",
    "                            mean = mean + (num_points-mean)/k\n",
    "                            std = std + (num_points-mean)*(num_points-old_mean)\n",
    "                            self.table[1][mapped_class_label] = mean\n",
    "                            self.table[2][mapped_class_label] = std\n",
    "    #@nb.njit\n",
    "    def count_rcs_class(self):\n",
    "        for line, cur_seq in enumerate(self.list_sequence):\n",
    "            timestamps = get_timestamps(cur_seq)\n",
    "            # iterate over snippet\n",
    "            list_start_idx = self.list_start_idx[line]\n",
    "            list_num_frame = self.list_num_future_frames[line]\n",
    "            for i, start in enumerate(list_start_idx):\n",
    "                num_frame = list_num_frame[i]\n",
    "                snip = get_frames(cur_seq, start, \n",
    "                            timestamps, n_next_frames=num_frame)\n",
    "                snip = clip(snip)\n",
    "                # iterate over clusters\n",
    "                track_ids = set(snip[\"track_id\"])\n",
    "                for tr_id in track_ids:\n",
    "                    if tr_id != b'':\n",
    "                        idx = where(snip[\"track_id\"] == tr_id)[0]\n",
    "                        #num_points = len(idx) # number of points\n",
    "                        class_label = snip[idx[0]]['label_id']\n",
    "                        mapped_class_label = ClassificationLabel.label_to_clabel(class_label).value\n",
    "                        if mapped_class_label != 5:\n",
    "                            for i in idx:\n",
    "                                max_rcs = self.rcs_table[3][mapped_class_label]\n",
    "                                min_rcs = self.rcs_table[4][mapped_class_label]\n",
    "                                mean_rcs = self.rcs_table[1][mapped_class_label]\n",
    "                                std_rcs = self.rcs_table[2][mapped_class_label]\n",
    "                                rcs = snip[i]['rcs']\n",
    "                                if max_rcs < rcs:\n",
    "                                    self.rcs_table[3][mapped_class_label] = rcs\n",
    "                                if min_rcs > rcs:\n",
    "                                    self.rcs_table[4][mapped_class_label] = rcs\n",
    "                                # get number of points\n",
    "                                self.rcs_table[0][mapped_class_label] += 1 \n",
    "                                k = self.rcs_table[0][mapped_class_label]\n",
    "                                old_mean_rcs = mean_rcs\n",
    "                                mean_rcs = mean_rcs + (rcs-mean_rcs)/k\n",
    "                                std_rcs = std_rcs + (rcs-mean_rcs)*(rcs-old_mean_rcs)\n",
    "                                self.rcs_table[1][mapped_class_label] = mean_rcs\n",
    "                                self.rcs_table[2][mapped_class_label] = std_rcs\n",
    "\n",
    "    def get_status(self):\n",
    "        print('************number of ponits*************')\n",
    "        print('CAR', 'PEDESTRIAN', 'PEDESTRIAN_GROUP', 'TWO_WHEELER', 'LARGE_VEHICLE')\n",
    "        print('max', self.table[3])\n",
    "        print('min', self.table[4])\n",
    "        print('mean', self.table[1])\n",
    "        a = self.table.view((float, len(self.table.dtype.names)))\n",
    "        print('standard deviation', sqrt(a[2]/(a[0]-1)))\n",
    "        print('************RCS*************')\n",
    "        print('max', self.rcs_table[3])\n",
    "        print('min', self.rcs_table[4])\n",
    "        print('mean', self.rcs_table[1])\n",
    "        a = self.rcs_table.view((float, len(self.rcs_table.dtype.names)))\n",
    "        print('standard deviation', sqrt(a[2]/(a[0]-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************number of ponits*************\n",
      "CAR PEDESTRIAN PEDESTRIAN_GROUP TWO_WHEELER LARGE_VEHICLE\n",
      "max (295., 75., 213., 117., 680.)\n",
      "min (3., 3., 3., 3., 3.)\n",
      "mean (37.56819363, 16.49654944, 30.80123402, 28.27210584, 97.48299792)\n",
      "standard deviation [33.16101831  9.34551214 22.74881082 16.70445649 87.8985989 ]\n",
      "************RCS*************\n",
      "max (44.6083374, 28.58427811, 35.9949646, 37.66812134, 49.97909546)\n",
      "min (-30.59911728, -30.59527206, -30.59353256, -30.56733513, -30.58673286)\n",
      "mean (-3.87777476, -8.35410227, -7.07877276, -10.30983934, 0.05502177)\n",
      "standard deviation [11.3356935   6.77332734  6.71474572  7.15346528 10.51745426]\n"
     ]
    }
   ],
   "source": [
    "snip_path = 'static/train_full.txt'\n",
    "dataset_path = \"/home/s0001516/thesis/dataset/RadarScenes_GPU/train_full\"\n",
    "dataset = CountDataset(dataset_path, snip_path)\n",
    "dataset.count_points_class()\n",
    "dataset.count_rcs_class()\n",
    "dataset.get_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average vr for static points: 0.16146290864516225\n",
      "max vr for static points: 114.888985\n"
     ]
    }
   ],
   "source": [
    "from labels import ClassificationLabel\n",
    "from dataset import BasicDataset\n",
    "from frame import get_frames, get_timestamps\n",
    "from numpy import ndarray, zeros, where, savetxt, sum, array, sqrt\n",
    "#from matplotlib import pyplot as plt\n",
    "from snippet import clip\n",
    "import numba as nb\n",
    "\n",
    "class StaticDataset(BasicDataset): \n",
    "    #@nb.njit\n",
    "    def count_static_class(self):\n",
    "        sum_static_vr = 0\n",
    "        sum_num_static = 0\n",
    "        max_vr = 0\n",
    "        for line, cur_seq in enumerate(self.list_sequence):\n",
    "            timestamps = get_timestamps(cur_seq)\n",
    "            # iterate over snippet\n",
    "            list_start_idx = self.list_start_idx[line]\n",
    "            list_num_frame = self.list_num_future_frames[line]\n",
    "            for i, start in enumerate(list_start_idx):\n",
    "                num_frame = list_num_frame[i]\n",
    "                snip = get_frames(cur_seq, start, \n",
    "                            timestamps, n_next_frames=num_frame)\n",
    "                snip = clip(snip)\n",
    "                static_id = where(snip['label_id'] == 11)[0]\n",
    "                static_vr = snip[static_id]['vr_compensated']\n",
    "                num_static = len(static_id)\n",
    "                biggest_vr = max(static_vr)\n",
    "                if biggest_vr > max_vr:\n",
    "                    max_vr = biggest_vr\n",
    "                sum_static_vr += sum(static_vr)\n",
    "                sum_num_static += sum(num_static)\n",
    "        mean_static_vr = sum_static_vr / sum_num_static    \n",
    "        print('average vr for static points:', mean_static_vr)\n",
    "        print('max vr for static points:', max_vr)\n",
    "\n",
    "\n",
    "snip_path = 'static/train_full.txt'\n",
    "dataset_path = \"/home/s0001516/thesis/dataset/RadarScenes_GPU/train_full\"\n",
    "dataset = StaticDataset(dataset_path, snip_path)\n",
    "dataset.count_static_class()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "54a171e7616625e97d9b27df94dc22b71fd8dcad61267e3bb570989eba1fca2c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('pytorchyolo-7wtUQjdd-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
